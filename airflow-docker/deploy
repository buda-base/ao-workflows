#!/usr/bin/env bash
#
#
# Create a deployment directory for the airflow docker compose service

#
# also cobbled from bdrc-docker.sh
# Create a test environment directory structure
# Args: path to parent.
# Result: Parent/Archive0/00-24
#               /Archive1/25-49
#               /Archive2/50-74
#               /Archive1/75-99

set -e
export ME=$(basename $0)
. ~/bin/init_sys.sh

# ------------------ CONFIG CONSTANTS ------------------
# don't edit these unless you must. Change the config in the
# DEV|PROD CONFIG section below
# prod:
_PROD_SYNC_ACCESS_UID=1001
_DEV_SYNC_ACCESS_UID=5000

_PROD_ARCH_ROOT=
_DEV_ARCH_ROOT=.
# ------------------ / CONFIG CONSTANTS ------------------

# ------------------ DEV|PROD CONFIG ------------------
ARCH_ROOT=$_PROD_ARCH_ROOT

SYNC_ACCESS_UID=$_PROD_SYNC_ACCESS_UID
# ------------------ /DEV|PROD CONFIG ------------------

function usage() {
  echo "Usage: ${ME} [-h|--help] -s|--source <source-dir> -d|--dest <deploy-dir> [-i|--init-env <deploy-dir>]"
  echo "Create and deployment directory for the airflow docker compose service"
  echo "  -h|--help"
  echo "  -s|--source <source-dir>: source directory"
  echo "  -d|--dest <deploy-dir>: deployment directory"
  echo "  -i|--init-env <deploy-dir>: initialize test environment AFTER creating it with --s and --d"
  echo ""
  echo "the -i|--init-env is used standalone to build a local copy of the RS archive for testing."
  echo "You need to manually reference its output in the bdrc-docker-compose.yaml scheduler:volumes:"
}
init_test_env() {

  # Make archive roots
  for d in $(seq 0 99); do
    todo=${1}/Archive$(($d / 25))/$(printf "%02d" $d)
    mkdir -p $todo || exit 1
    chmod -R 777 $todo
  done

  # Make download target
  todo=${1}/AO-staging-Incoming/bag-download
  mkdir -p $todo || exit 1
  chmod -R 777 ${1}/AO-staging-Incoming
}

OPTIONS=$(getopt -o hs:d:i: --long help,source,dest,init-env -- "$@")

while true; do # Parse command line options
  case "$1" in
  -s | --source)
    export SRC=${2?"No source directory given."}
    shift 2
    ;;
  -d | --dest)
    export DEPLOY=${2?"No deployment directory given."}
    shift 2
    ;;
  -i | --init-env)
    init_home=${2?"No archival parent directory given."}
    init_test_env $2
    exit $?
    ;;
  -h | --help)
    usage
    exit 1
    ;;

  --)
    shift
    break
    ;;
  *)
    break
    ;;
  esac
done

if [[ -z $SRC ]] || [[ -z $DEPLOY ]]; then
  echo no args. bye
  exit 1
fi

if [[ ! -d $DEPLOY ]]; then
  mkdir $DEPLOY
  # jimk aow-14 - add sync logs
  for DD in "$DEPLOY/dags" "$DEPLOY/logs"  "$DEPLOY/plugins" "$DEPLOY/processing" "$DEPLOY/processing/logs" ; do
    mkdir -p $DD
    chmod 777 $DD
  done
fi

log_echo docker-compose
cp $SRC/bdrc-docker-compose.yml $DEPLOY/docker-compose.yaml
log_echo dags
cp -r $SRC/dags ${DEPLOY}

# Borrowed from bdrc-docker.sh
export SECRETS=$DEPLOY/.docker-secrets
mkdir -p "${SECRETS}"
log_echo secrets - bdrc
cp -v ~/.config/bdrc/docker/* "${SECRETS}" || exit 1
#
# These section names, 'default' and 'ap_northeast', are hard-coded in the dags/..py
log_echo aws default
./extract-section.pl ~/.aws/credentials default > ${SECRETS}/aws-credentials || exit 1
# jimk archive-ops-14 - support other regions
log_echo aws-others
./extract-section.pl ~/.aws/credentials ap_northeast >> ${SECRETS}/aws-credentials || exit 1
# chmod u-w "${SECRETS}"

log_echo .env
cat <<EOF >$DEPLOY/.env
SECRETS=.docker-secrets
# hush run-time warnings of build environments
COMPOSE_PY_REQS=
BIN=
COMPOSE_BUILD_DIR=
ARCH_ROOT=$ARCH_ROOT
#
# For build only
BUILD_CONFIG_ROOT=

SYNC_ACCESS_UID=$SYNC_ACCESS_UID
EOF

# They say this is a BUILD image arg, not a runtime arg
# AIRFLOW_UID=1000 # j...
# AIRFLOW_GID=0

log_echo "Deployment directory created: $DEPLOY"
log_echo "manage  the services with: cd $DEPLOY && docker compose [up -d | down | log | exec....."
