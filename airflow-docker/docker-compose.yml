version: '3.7'

# ====================================== AIRFLOW ENVIRONMENT VARIABLES =======================================
x-environment: &airflow_environment
  - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
  - AIRFLOW__CORE__LOAD_EXAMPLES=False
  - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@postgres:5432/airflow
  - AIRFLOW__CORE__STORE_DAG_CODE=True
  - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
  - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True

# x-airflow-image: &airflow_image apache/airflow:2.0.0-python3.8
# jimk - lets get frisky
# Origx-airflow-image: &airflow_image apache/airflow:2.8.1-python3.11
# See ./dockerfile
x-airflow-image: &airflow_image bdrc-airflow
#
# Bit of a bust
#x-pycharm-helper: &pycharm_helper pycharm_helpers:PY-233.11799.298
##x-pycharm-env: &pycharm_env
##  - PYCHARM_DEBUG=True
##  - PYCHARM_DEBUG_PORT=12345
##  - PYCHARM_DEBUG_HOST=host.docker.internal
##  -
x-dbrc-log: &bdrc_log qa
# ====================================== /AIRFLOW ENVIRONMENT VARIABLES ======================================
services:
  postgres:
    image: postgres:12-alpine
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"

  init:
    image: *airflow_image
    depends_on:
      - postgres
    environment: *airflow_environment
    entrypoint: /bin/bash
    command: -c 'airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org'

  webserver:
    image: *airflow_image
    restart: always
    depends_on:
      - postgres
    ports:
      - "8089:8080"
    volumes:
      # DEV/TEST use bind mounts
      - /Users/jimk/dev/tmp/Projects/airflow/syslog:/opt/airflow/logs
    environment: *airflow_environment
    command: webserver

  scheduler:
    image: *airflow_image
    restart: always
    depends_on:
      - postgres
    volumes:
      # DEV/TEST use bind mounts
      # System logging - /opt/airflow/logs is the airflow system default
      # TODO: Migrate to debian
      - /Users/jimk/dev/tmp/Projects/airflow/syslog:/opt/airflow/logs
      # TODO: Migrate to debian
      # App logging. Replicate in glacier_staging_to_sync.py
      - /Users/jimk/dev/tmp/Projects/airflow/log:/home/airflow/log
      # We don't need to persist stating in a resource shared between containers
      # - staging:/home/airflow/staging
      # Export actual sync product - this is wrto bodhi/sattva
#      - /mnt/Archive0:/var/mnt/Archive0
#      - /mnt/Archive1:/var/mnt/Archive1
#      - /mnt/Archive2:/var/mnt/Archive2
#      - /mnt/Archive3:/var/mnt/Archive3
    # For testing on local mac. This is a good reason for not
    # using files, but a service
      - /Users/jimk/dev/tmp/Archive0:/home/airflow/extern/Archive0
      - /Users/jimk/dev/tmp/Archive1:/home/airflow/extern/Archive1
      - /Users/jimk/dev/tmp/Archive2:/home/airflow/extern/Archive2
      - /Users/jimk/dev/tmp/Archive3:/home/airflow/extern/Archive3
# This is for pyCharm development only.
# DEBUG:
      - /Users/jimk/dev/ao-workflows/StagingGlacierProcess:/opt/airflow/dags
    environment: *airflow_environment
    command: scheduler

    secrets:
      - db_apps
      - drs_cnf
      - aws

secrets:
  db_apps:
    file:
      .secrets/db_apps.config
  drs_cnf:
    file:
      .secrets/drs.config
  aws:
    file:
      .secrets/aws-credentials

volumes:
  staging:
  logs:
